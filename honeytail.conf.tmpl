[Application Options]
; Only send 1 / N log lines
; SampleRate = 1

; Number of concurrent connections to open to Honeycomb
; NumSenders = 80

; How frequently to flush batches
; BatchFrequencyMs = 100

; Maximum number of messages to put in a batch
; BatchSize = 50

; Print debugging output
; Debug = false

; Instead of sending events to Honeycomb, print them to STDOUT for debugging
; DebugOut = false

; How frequently, in seconds, to print out summary info
; StatusInterval = 60

; Configure honeytail to ingest old data in order to backfill Honeycomb. Sets the correct values for --backoff, --tail.read_from, and --tail.stop
; Backfill = false

; When backfilling data, rebase timestamps relative to the current time.
; RebaseTime = false

; When parsing a timestamp that has no time zone, assume it is in the same timezone as localhost instead of UTC (the default)
; Localtime = false

; When parsing a timestamp use this time zone instead of UTC (the default). Must be specified in TZ format as seen here: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
; Timezone =

; For the field listed, apply a one-way hash to the field content. May be specified multiple times
; ScrubFields =

; Do not send the field to Honeycomb. May be specified multiple times
; DropFields =

; Add the field to every event. Field should be key=val. May be specified multiple times
; AddFields =

; Data Augmentation Map file. Path to a file that contains JSON mapping of columns to augment, the values of the column, and new objects to be inserted into the event, eg to add hostname based on IP address or username based on user ID
; DAMapFile =

; Identify a field that contains an HTTP request of the form 'METHOD /path HTTP/1.x' or just the request path. Break apart that field into subfields that contain components. May be specified multiple times. Defaults to 'request' when using the nginx parser
; RequestShape =

; Prefix to use on fields generated from request_shape to prevent field collision
; ShapePrefix =

; A pattern for the request path on which to base the derived request_shape. May be specified multiple times. Patterns are considered in order; first match wins.
; RequestPattern =

; How to parse the request query parameters. 'whitelist' means only extract listed query keys. 'all' means to extract all query parameters as individual columns
; RequestParseQuery = whitelist

; Request query parameter key names to extract, when request_parse_query is 'whitelist'. May be specified multiple times.
; RequestQueryKeys =

; When rate limited by the API, back off and retry sending failed events. Otherwise failed events are dropped. When --backfill is set, it will override this option=true
; BackOff = false

; pass a regex to this flag to strip the matching prefix from the line before handing to the parser. Useful when log aggregation prepends a line header. Use named groups to extract fields into the event.
; PrefixRegex =

; Specify a field to deterministically sample on, i.e., every concurrent Honeytail instance will sample 1/N based on content.
; DeterministicSample =

; enable dynamic sampling using the field listed in this option. May be specified multiple times; fields will be concatenated to form the dynsample key. WARNING increases CPU utilization dramatically over normal sampling
; DynSample =

; measurement window size for the dynsampler, in seconds
; DynWindowSec = 30

; If this log has already been sampled, specify the field containing the sample rate here and it will be passed along unchanged
; PreSampledField =

; if the rate of traffic falls below this, dynsampler won't sample
; MinSampleRate = 1

; JSON fields encoded as string to unescape and properly parse before sending
; JSONFields =

; Log file(s) to exclude from --file glob. Can specify multiple times, including multiple globs.
; FilterFiles =

[Required Options]
; Parser module to use. Use --list to list available options.
ParserName = json

; Team write key
WriteKey = ${HONEYCOMB_KEY}

; Log file(s) to parse. Use '-' for STDIN, use this flag multiple times to tail multiple files, or use a glob (/path/to/foo-*.log)
LogFiles = -

; Name of the dataset
Dataset = signup-bot

[Other Modes]
; Show this help message
; Help = false

; List available parsers
; ListParsers = false

; Show version
; Version = false

[Tail Options]
; Location in the file from which to start reading. Values: beginning, end, last. Last picks up where it left off, if the file has not been rotated, otherwise beginning. When --backfill is set, it will override this option=beginning
; ReadFrom = last

; Stop reading the file after reaching the end rather than continuing to tail. When --backfill is set, it will override this option=true
; Stop = false

; use poll instead of inotify to tail files
; Poll = false

; File in which to store the last read position. Defaults to a file in /tmp named $logfile.leash.state. If tailing multiple files, default is forced.
; StateFile =

[JSON Parser Options]
; Name of the field that contains a timestamp
TimeFieldName = timestamp

; Format of the timestamp found in timefield (supports strftime and Golang time formats)
TimeFieldFormat = 2006-01-02T15:04:05.999999999Z07:00
